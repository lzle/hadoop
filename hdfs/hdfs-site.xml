<!-- 每个 DataNode 复制副本并发数 -->
<property>
<name>dfs.namenode.replication.max-streams</name>
<value>6</value>
</property>

<property>
<name>dfs.namenode.replication.max-streams-hard-limit</name>
<value>10</value>
</property>

<!-- 每次筛选副本数 = numlive * this -->
<property>
<name>dfs.namenode.replication.work.multiplier.per.iteration</name>
<value>10</value>
</property>

<!-- 心跳重新检查时间(ms),超过两倍的时间判断节点失效 -->
<property>
<name>dfs.namenode.heartbeat.recheck-interval</name>
<value>900000</value>
</property>

<!-- 每次筛选无效副本数 = Math.ceil(numlive * this) -->
<property>
<name>dfs.namenode.invalidate.work.pct.per.iteration</name>
<value>0.32f</value>
</property>

<!-- 每个心跳删除命令由 NameNode 发送到 DataNode 的最大无效块数 -->
<property>
<name>dfs.block.invalidate.limit</name>
<value>16</value>
</property>

<!-- NameNode 处理 RPC 请求的线程数，建议值 ln(number of DataNodes in this HDFS service) * 20 -->
<property>
<name>dfs.namenode.handler.count</name>
<value>150</value>
</property>

<!-- DataNode 处理 RPC 请求的线程数，处理 NameNode 请求 -->
<property>
<name>dfs.datanode.handler.count</name>
<value>10</value>
</property>

<!-- DataNode 处理数据传输的线程数 -->
<property>
<name>dfs.datanode.max.transfer.threads</name>
<value>65535</value>
</property>

<!-- 关闭本地写 -->
<property>
<name>dfs.namenode.block-placement-policy.default.prefer-local-node</name>
<value>false</value>
</property>

<!-- 优先筛选更低使用率的 DataNode -->
<property>
  <name>dfs.block.replicator.classname</name>
  <value>org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy</value>
</property>

<property>
  <name>dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction</name>
  <value>0.6</value>
  <description>
    Special value between 0 and 1, noninclusive.  Increases chance of
    placing blocks on Datanodes with less disk space used.
  </description>
</property>


<!-- FSImageFile 存储地址 -->
<property>
<name>dfs.namenode.name.dir</name>
<value>/opt/hadoop/data/dfs/nn</value>
<final>false</final>
<source>hdfs-site.xml</source>
</property>

<!-- EditLogFile 存储地址 -->
<property>
<name>dfs.namenode.edits.dir</name>
<value>${dfs.namenode.name.dir}</value>
<final>false</final>
<source>hdfs-default.xml</source>
</property>
